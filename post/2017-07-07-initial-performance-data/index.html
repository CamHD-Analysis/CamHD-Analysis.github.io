<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>CamHD Video Analysis  | First performance benchmarks from scaled-out processing</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.56.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    <link href='https://camhd-analysis.github.io/dist/main.css' rel='stylesheet' type="text/css" />
    
      
    

    

    <meta property="og:title" content="First performance benchmarks from scaled-out processing" />
<meta property="og:description" content="While I&rsquo;ve been working away on documentation on my scaled-out image analysis procedure, my swarms have been humming away in the background, working through the CamHD archive. As of today (July 7 2017), I&rsquo;ve done optical flow processing on the bulk of 2016, with 2017 in the queue. The region analysis still requires careful quality control, so I haven&rsquo;t started it yet. But it requires far less processing time.
Before I start throwing out numbers, let me briefly outline how the processing works." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://camhd-analysis.github.io/post/2017-07-07-initial-performance-data/" />
<meta property="article:published_time" content="2017-07-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2017-07-07T00:00:00+00:00" />
<meta itemprop="name" content="First performance benchmarks from scaled-out processing">
<meta itemprop="description" content="While I&rsquo;ve been working away on documentation on my scaled-out image analysis procedure, my swarms have been humming away in the background, working through the CamHD archive. As of today (July 7 2017), I&rsquo;ve done optical flow processing on the bulk of 2016, with 2017 in the queue. The region analysis still requires careful quality control, so I haven&rsquo;t started it yet. But it requires far less processing time.
Before I start throwing out numbers, let me briefly outline how the processing works.">


<meta itemprop="datePublished" content="2017-07-07T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2017-07-07T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1059">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="First performance benchmarks from scaled-out processing"/>
<meta name="twitter:description" content="While I&rsquo;ve been working away on documentation on my scaled-out image analysis procedure, my swarms have been humming away in the background, working through the CamHD archive. As of today (July 7 2017), I&rsquo;ve done optical flow processing on the bulk of 2016, with 2017 in the queue. The region analysis still requires careful quality control, so I haven&rsquo;t started it yet. But it requires far less processing time.
Before I start throwing out numbers, let me briefly outline how the processing works."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://camhd-analysis.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      CamHD Video Analysis
    </a>
    <div class="flex-l items-center">
      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Posts page">
              Posts
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/page/" title="Pages page">
              Pages
            </a>
          </li>
          
        </ul>
      
      








    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  <div class="flex-l mt2 mw8 center">
    <article class="center cf pv5 ph3 ph4-ns mw7">
      <p class="f6 b helvetica tracked">
        
        POST
      </p>
      <h1 class="f1 athelas">
        First performance benchmarks from scaled-out processing
      </h1>
        
        
      <time class="f6 mv4 dib tracked" datetime="2017-07-07T00:00:00Z">
        July 7, 2017
      </time>
      <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray">
        <p>While I&rsquo;ve been working away on <a href="{{ site.baseurl }}{% post_url 2017-06-16-scaling-up-frame-analysis-part-one %}">documentation</a> on my <a href="{{ site.baseurl }}{% post_url 2017-07-05-scaling-up-frame-analysis-part-two %}">scaled-out image</a>
<a href="{{ site.baseurl }}{% post_url 2017-07-06-scaling-up-frame-analysis-part-three %}">analysis procedure</a>,
my swarms have been humming away in the background, working through the CamHD archive.   As of today (July 7 2017), I&rsquo;ve
done optical flow processing on the bulk of 2016, with 2017 in the queue.   The region analysis still
requires careful quality control, so I haven&rsquo;t started it yet.   <em>But</em> it requires far less processing time.</p>

<p>Before I start throwing out numbers, let me briefly outline how the processing
works.  It&rsquo;s important to recognize the multiple places where threading is used
to max out throughput. I don&rsquo;t pretend this is an optimal, and I&rsquo;m sure I could
reduce the computational cost</p>

<p>One &ldquo;job&rdquo; is the optical flow processing of one movie.   This requires
performing an expensive optical flow calculation on every 10&rsquo;th frame in the
movie. From the top down, then:</p>

<ol>
<li>The RQ worker is written in Python.  It receives a movie from the work queue, and pulls the movie metadata from Lazycache.</li>
<li>It then uses <code>dask.threaded</code> to schedule an array of jobs across all of the local cores, one per frame to be analyzed.</li>
<li>For each frame, an instance of the <code>frame_stats</code> C++ program:

<ol>
<li>Retrieves two frames from Lazycache &hellip; this will be a slow IO wait.</li>
<li>Performs optical flow using (currently) the CPU implementations of <a href="http://docs.opencv.org/2.4.13.2/modules/video/doc/motion_analysis_and_object_tracking.html#createoptflow-dualtvl1">DualLTV1</a> in OpenCV 2.4.x.   Internally, this uses OpenCV&rsquo;s <a href="https://github.com/opencv/opencv/blob/master/modules/core/src/parallel.cpp"><code>parallel_for_</code></a> abstraction to perform multithreaded loop unrolling.   I use the standard <em>yakkety</em> Ubuntu packages for OpenCV, which I <em>believe</em> means <a href="https://www.threadingbuildingblocks.org">TBB</a> is used (it&rsquo;s the first choice in the <code>#ifdef</code> tree).<br /></li>
<li>Performs an optimization using <a href="http://ceres-solver.org">Ceres</a>.  I believe Ceres will use multiple threads to evaluate the Jacobian, though I&rsquo;m not sure if it&rsquo;s necessary in this case.</li>
<li>Does some other bookkeeping, resulting in the frame&rsquo;s results being stored as JSON.</li>
<li>Dask gathers those per-frame JSON results, and generates the whole-movie <a href="https://github.com/CamHD-Analysis/CamHD_motion_metadata/blob/master/docs/OpticalFlow.md"><code>optical_flow.json</code></a> file.</li>
</ol></li>
</ol>

<p>All of this is done within one <a href="{{ site.baseurl }}{%
post_url 2017-07-05-scaling-up-frame-analysis-part-two %}">Docker &ldquo;worker&rdquo; image</a>.   I run two workers
and a lazycache container on every computer in the cluster.   This means each PC
is heavily overloaded, but given the  number of IO waits while communicating
with CI, and other single-threaded code segments, this ensures all system cores
are kept busy, at some task-switching expense.</p>

<p>Based on all this, I assert the processing is network and CPU bound.  Memory usage and disk IO are negligible.</p>

<p>I have two clusters.  My &ldquo;desktop&rdquo; cluster consists of three Core i7 machines of different generations.   My Google cloud consists of eight preemptible instances of their <a href="https://cloud.google.com/compute/pricing#predefined_machine_types"><code>n1-highcpu-8</code></a> running in their <code>us-central1-a</code> zone.   The cap of eight instances is essentially arbitrary &ndash; new projects have a default quota of 72 cores,
but I also am running a 1-CPU, non-preemptible <code>n1-standard-1</code> instances as swarm manager, Redis server and file server for the cluster.  The preemptible instances save significant running costs but this cluster needs to be refreshed on a daily basis.   This should be straightforward to automate, at least to a minimal level of robustness, but I haven&rsquo;t done it yet.</p>

<p>In terms of specs, my Google cloud machines cost:</p>

<table>
<tr><th>Type</th><th>Virtual CPUs</th><th>Memory (GB)</th><th>Price (USD/hour)</th><th>Notes</th></tr>
<tr><td>n1-standard-1</td><td>1</td><td>3.75</td><td>$0.0100</td><td>Up to 60% discount for <a href="https://cloud.google.com/compute/docs/sustained-use-discounts">sustained use</a></td></tr>
<tr><td>n1-highcpu-8</td><td>8</td><td>7.2</td><td>$0.0600</td><td>Preemptible price</td></tr>
<tr><td></td><td></td><td></td><td>$0.2836</td><td>Standard pricing (FWIW)</td></tr>
</table>

<p>The default processor in the <code>us-central1-a</code> zone is a 2.6GHz Xeon E5 &ldquo;Sandy Bridge.&rdquo;   There are also Ivy Bridge, Broadwell and Skylake machines available, but I haven&rsquo;t experimented with selecting these newer processors.    At present, only Skylake processors carry a price premium.</p>

<p>These machines also incur a disk usage charge of $0.04 / GB / month for their boot disks, which turns out to be pretty negligible.   This processing relies almost exclusively on network ingress (which is free) and intra-zone communications.</p>

<p>** Early on, I used my public Lazycache instances for all processing.  Besides being slower, I also racked up significant egress charges sending images from GAE to my desktop cluster!</p>

<p>Here&rsquo;s the data, spanning 2087 files &mdash; this is not quite all of the videos from 2016.  All times are given as <strong>mean (std dev)</strong></p>

<table>
<tr><th>CPU</th><th>Threads</th><th>Instances</th><th>Num movies</th><th>Sec. per movie</th><th>Net sec. per frame</th><th>Actual sec. per frame</th></tr>
<tr><td>Core i7-3770K @ 3.5GHz</td><td>8</td><td>1</td><td>263</td><td>5140.33 (1463.13)</td><td>0.21462 (0.04453)</td><td>16.613 (7.535)</td></tr>
<tr><td>Core i7-5820K @ 3.3GHz</td><td>12</td><td>1</td><td>400</td><td>3370.24 (908.48)</td><td>0.14181 (0.02809)</td><td>16.294 (7.584)</td></tr>
<tr><td>Core i7-6700K @ 4.0GHz</td><td>8</td><td>1</td><td>459</td><td>3086.79 (979.91)</td><td>0.12857 (0.03259)</td><td>9.976 (4.959)</td></tr>
<tr><td>Xeon "Sandy Bridge" @ 2.60GHz</td><td>8</td><td>8</td><td>965</td><td>6220.31 (2809.39)</td><td>0.26655  (0.10460)</td><td>20.469 (12.314)</td></tr>
</table>

<p><strong>Threads</strong> includes Hyper-threading for the i7 processors (twice the number of physical cores), and is the number of &ldquo;virtual CPUs&rdquo; for the cloud compute instances.</p>

<p><strong>Num movies</strong> is the number of movies in the sample.  Don&rsquo;t read too much into this number, as the Google cluster was not running for some portion of the processing, so it is undercounted in the final percentages.</p>

<p><strong>Sec. per movie</strong> is the total wall time required to process one movie.   Movies <strong>can</strong> be of different length, but the majority of the sample is the standard 12-13 minute length.</p>

<p><strong>Net sec. per frame</strong> is <em>(total wall time)/(number of frames)</em>, giving the aggregate throughput of a worker.</p>

<p><strong>Actual sec. per frame</strong> is the wall clock time spent analyzing a single frame.  This number is much larger than the net seconds per frame because Dask schedules multiple frames simultaneously.</p>

<p>At this rate, processing a single video on a single cloud  instance requires (on average) 10.37 cents.   Processing all of 2016 would require approx. $238 and 165-processor-days, assuming zero idle time on the virtual machines, and disregarding the cost of the swarm manager node at ~$25/month.   Notably, this cost is <strong>time independent</strong>, I can run my eight instances for 21 days, or 64 instances (512 vCPUs) for 2.6 days <em>for the same cost</em> (assuming you can requisition 64 instances, of course).</p>

<p>FWIW, here are the <a href="https://www.cpubenchmark.net/">Passmark CPU benchmarks</a> for these processors:</p>

<table>
<tr><th>CPU</th><th>Single Thread mark</th><th>CPU Mark</th></tr>
<tr><td>Core i7-3770K</td><td>2084</td><td>9547</td></tr>
<tr><td>Core i7-5820K</td><td>2016</td><td>12995</td></tr>
<tr><td>Core i7-6700K</td><td>2349</td><td>11110</td></tr>
<tr><td>Xeon "Sandy Bridge" @ 2.60GHz**</td><td>1591</td><td>12275</td></tr>
</table>

<p>** Using the <a href="http://ark.intel.com/products/64595">Xeon E5-2670</a> which has a base freq of 2.6GHz, 8 cores and 16 threads.   To a casual analysis,
this works out.  The cloud instance, running on 8 virtual CPUs, or &ldquo;half&rdquo; of the Xeon, requires approximately twice as long as the i7-5820k
which has a similar CPU mark.   Interesting, the i7-6700K, which is nominally a &ldquo;lesser&rdquo; machine than than the i7-5820k, due to the smaller number of physical cores (4 versus 6) is consistently faster in actual performance.  This may be because single-threaded computation remains an important part of the total computational time, or due to other non-compute tasks running on the i7-5820k (which is also my primary development machine).</p>

      </section>
      


    </article>
    <div class="ph3 mt2 mt6-ns">
      

    </div>
  </div>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://camhd-analysis.github.io/" >
    &copy; 2019 CamHD Video Analysis
  </a>
  








  </div>
</footer>

    <script src="https://camhd-analysis.github.io/dist/app.bundle.js" async></script>

  </body>
</html>
